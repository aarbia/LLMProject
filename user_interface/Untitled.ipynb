{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e261543a-47af-4ee2-9002-d502de73d11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_dir DATA_DIR] [--output_dir OUTPUT_DIR] [--n_epochs N_EPOCHS]\n",
      "                             [--print_sample_iter PRINT_SAMPLE_ITER] [--eval_freq EVAL_FREQ] [--save_ckpt_freq SAVE_CKPT_FREQ]\n",
      "                             [--lr LR] [--batch_size BATCH_SIZE] [--debug DEBUG]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\alexa\\AppData\\Roaming\\jupyter\\runtime\\kernel-1cb05fde-16e0-4956-ac01-c703d070eab2.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexa\\OneDrive\\Documents\\modernAI\\LLM\\.pixi\\envs\\default\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size, max_length, stride,\n",
    "                         shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "    return text_data\n",
    "\n",
    "\n",
    "def create_dataloaders(text_data, train_ratio, batch_size, max_length, stride, num_workers=0):\n",
    "    split_idx = int(train_ratio * len(text_data))\n",
    "    train_loader = create_dataloader_v1(\n",
    "        text_data[:split_idx],\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    val_loader = create_dataloader_v1(\n",
    "        text_data[split_idx:],\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def convert_time(seconds):\n",
    "    hours, rem = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    return int(hours), int(minutes), int(seconds)\n",
    "\n",
    "\n",
    "def print_eta(start_time, book_start_time, index, total_files):\n",
    "    book_end_time = time.time()  # End time of processing this book\n",
    "    elapsed_time = book_end_time - book_start_time\n",
    "    total_elapsed_time = book_end_time - start_time\n",
    "    books_remaining = total_files - index\n",
    "    average_time_per_book = total_elapsed_time / index\n",
    "    eta = average_time_per_book * books_remaining\n",
    "\n",
    "    book_h, book_m, book_s = convert_time(elapsed_time)\n",
    "    total_h, total_m, total_s = convert_time(total_elapsed_time)\n",
    "    eta_h, eta_m, eta_s = convert_time(eta)\n",
    "\n",
    "    print(f\"Book processed {book_h}h {book_m}m {book_s}s\"\n",
    "          f\"\\nTotal time elapsed {total_h}h {total_m}m {total_s}s\"\n",
    "          f\"\\nETA for remaining books: {eta_h}h {eta_m}m {eta_s}s\")\n",
    "\n",
    "\n",
    "def train_model_simple(model, optimizer, device, n_epochs,\n",
    "                       eval_freq, eval_iter, print_sample_iter, start_context,\n",
    "                       output_dir, save_ckpt_freq, tokenizer,\n",
    "                       batch_size=1024, train_ratio=0.90):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            # Iterate over the books in the training corpus\n",
    "            for index, file_path in enumerate(all_files, 1):\n",
    "                book_start_time = time.time()\n",
    "                text_data = read_text_file(file_path) + \" <|endoftext|> \"\n",
    "                print(f\"Tokenizing file {index} of {total_files}: {file_path}\")\n",
    "\n",
    "                # Initialize new data loaders for each book\n",
    "                train_loader, val_loader = create_dataloaders(\n",
    "                    text_data,\n",
    "                    train_ratio=train_ratio,\n",
    "                    batch_size=batch_size,\n",
    "                    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    num_workers=0\n",
    "                )\n",
    "                print(\"Training ...\")\n",
    "                model.train()\n",
    "                for input_batch, target_batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    tokens_seen += input_batch.numel()\n",
    "                    global_step += 1\n",
    "\n",
    "                    # Optional evaluation step\n",
    "                    if global_step % eval_freq == 0:\n",
    "                        train_loss, val_loss = evaluate_model(\n",
    "                            model, train_loader, val_loader, device, eval_iter)\n",
    "                        train_losses.append(train_loss)\n",
    "                        val_losses.append(val_loss)\n",
    "                        track_tokens_seen.append(tokens_seen)\n",
    "                        print(f\"Ep {epoch+1} (Step {global_step}): \"\n",
    "                              f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "                    # Generate text passage\n",
    "                    if global_step % print_sample_iter == 0:\n",
    "                        generate_and_print_sample(\n",
    "                            model, tokenizer, device, start_context\n",
    "                        )\n",
    "\n",
    "                if global_step % save_ckpt_freq:\n",
    "                    file_name = output_dir / f\"model_pg_{global_step}.pth\"\n",
    "                    torch.save(model.state_dict(), file_name)\n",
    "                    print(f\"Saved {file_name}\")\n",
    "\n",
    "                print_eta(start_time, book_start_time, index, total_files)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        file_name = output_dir / f\"model_pg_{global_step}_interrupted.pth\"\n",
    "        torch.save(model.state_dict(), file_name)\n",
    "        print(f\"Saved {file_name}\")\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='GPT Model Training Configuration')\n",
    "\n",
    "    parser.add_argument('--data_dir', type=str, default='text',\n",
    "                        help='Directory containing the training data')\n",
    "    parser.add_argument('--output_dir', type=str, default='model_checkpoints',\n",
    "                        help='Directory where the model checkpoints will be saved')\n",
    "    parser.add_argument('--n_epochs', type=int, default=1,\n",
    "                        help='Number of epochs to train the model')\n",
    "    parser.add_argument('--print_sample_iter', type=int, default=1000,\n",
    "                        help='Iterations between printing sample outputs')\n",
    "    parser.add_argument('--eval_freq', type=int, default=100,\n",
    "                        help='Frequency of evaluations during training')\n",
    "    parser.add_argument('--save_ckpt_freq', type=int, default=100_000,\n",
    "                        help='Frequency of saving model checkpoints during training')\n",
    "    parser.add_argument('--lr', type=float, default=5e-4,\n",
    "                        help='Learning rate for the optimizer')\n",
    "    parser.add_argument('--batch_size', type=int, default=4,\n",
    "                        help='Batch size for training')\n",
    "    parser.add_argument('--debug', type=bool, default=False,\n",
    "                        help='Uses a very small model for debugging purposes')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.debug:\n",
    "        GPT_CONFIG_124M = {\n",
    "            \"vocab_size\": 50257,     # Vocabulary size\n",
    "            \"context_length\": 10,    # Context length\n",
    "            \"emb_dim\": 12,           # Embedding dimension\n",
    "            \"n_heads\": 2,            # Number of attention heads\n",
    "            \"n_layers\": 2,           # Number of layers\n",
    "            \"drop_rate\": 0.0,        # Dropout rate, deactivated via 0.0 as dropout in LLMs is not recommended anymore\n",
    "            \"qkv_bias\": False        # Query-key-value bias\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        GPT_CONFIG_124M = {\n",
    "            \"vocab_size\": 50257,     # Vocabulary size\n",
    "            \"context_length\": 1024,  # Context length\n",
    "            \"emb_dim\": 768,          # Embedding dimension\n",
    "            \"n_heads\": 12,           # Number of attention heads\n",
    "            \"n_layers\": 12,          # Number of layers\n",
    "            \"drop_rate\": 0.1,        # Dropout rate\n",
    "            \"qkv_bias\": False        # Query-key-value bias\n",
    "        }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.1)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    data_dir = args.data_dir\n",
    "    all_files = [os.path.join(path, name) for path, subdirs, files\n",
    "                 in os.walk(data_dir) for name in files if name.endswith((\".txt\"))]\n",
    "    [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith(\".txt\")]\n",
    "    total_files = len(all_files)\n",
    "\n",
    "    if total_files == 0:\n",
    "        print(\"No training text files found. Make sure you \"\n",
    "              \"selected the correct input directory\")\n",
    "        quit()\n",
    "    print(\"Total files:\", total_files)\n",
    "\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "        model, optimizer, device,\n",
    "        batch_size=args.batch_size,\n",
    "        n_epochs=args.n_epochs,\n",
    "        eval_freq=args.eval_freq,\n",
    "        eval_iter=1,\n",
    "        print_sample_iter=args.print_sample_iter,\n",
    "        output_dir=output_dir,\n",
    "        save_ckpt_freq=args.save_ckpt_freq,\n",
    "        start_context=\"Every effort moves you\",\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    epochs_tensor = torch.linspace(0, args.n_epochs, len(train_losses))\n",
    "    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, output_dir)\n",
    "\n",
    "    torch.save(model.state_dict(), output_dir / \"model_pg_final.pth\")\n",
    "    print(f\"Maximum GPU memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634d3da-4a13-4929-9489-c93a70500e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "    \n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
